自分の性格上、得られた結果を見てすぐ新しい検証に手が出てしまうのですが、その結果、過去の結果が雑に保管されてしまい、自分が想定していたデータセットではないものを学習に使用していて、結果を見て「あれー！？」みたいなことがありました。。
特にデータセットもjsonlファイルで、スクリプト内で自動生成してアップロードする形にしていたので、中身をちゃんと確認せずに作ってしまっていたのもよくないですね。
ここら辺で数十ドルと1日を無駄にしてしまったので、一度立ち止まって実験管理をしっかりしたいなと思いました！
OpenAIがFine-tuning用のUIを作ってくれる予定らしいので、実験管理周りもぜひ充実してもらいたいところです…！

## おわりに
今回のFine-tuningでの学びを改めて、箇条書きでまとめてみます！
Fine-tuningをすると強めに出力形式を強制してくれる。そのためFunction Callingよりも強力に形式を指定したい場合に有効そう
epoch数が多ければ品質向上するわけではなく、個々のケースにおいて最適解がありそう。OpenAIは1~2回ずつ変えながら調整してみてと言ってる
学習用データセットによってFine-tuningモデルの品質はかなり変わる。「その会話集は、通常のgpt-3.5-turboとの会話でもうまくいきそうか？」みたいな視点を持っておくと良いかもしれない
実験管理はちゃんとしよう！ファイルの中身はちゃんと確認し、ファイル名やモデル名には識別子を設定して、差分が何かをドキュメントにまとめておこう！
2日で120ドルは飛んで行きましたが、それ相応の学びはあった気がするので良しとします！💸（Fine-tuningを15回くらいしました）

この記事は、2023/8/30のLLM Application Vol.2でのLT登壇での補完記事となっています。


